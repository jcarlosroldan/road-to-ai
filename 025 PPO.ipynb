{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `018` Reinforcement Learning\n",
    "\n",
    "Requirements: 016 Transformers\n",
    "\n",
    "Neural networks are, by definition, an universal function approximator. And due to the nature of functions, they are expected to solve classification/regression problems in which we have an input, and we expect an single output.\n",
    "\n",
    "However, the real world doesn't always work like this. In many occasions we have time-continuous problems, in which we need some agent capable of interacting with the environment multiple times until reaching the desired goal. For instance, to train a neural network to play tic-tac-toe, we need to define a loss function that gets lower the better the network is.\n",
    "\n",
    "Reinforcement learning (RL) was born as a solution to this problem. In RL, we define a neural network that we call an agent, which is trained to interact with an environment. The agent receives observations from the environment (in the tic-tac-toe context that'd be the pieces in the board), and outputs the logits of the actions it can take. The environment then executes one of the actions based on the logits, and returns the new observation (the new board state) and a reward (a number that tells the agent how well it's doing).\n",
    "\n",
    "Note that this example defines how RL works on a discrete problem (we have a finite number of actions) with a stochastic environment transition (the same action might lead to different outcomes based on how the opponent reacts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.backends.cudnn.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an environment for the game. Any structure that contains a board initialization, a method to execute an action and a method to check if the game is over will do. We will use a simple class-based format that is very similar to the most standard (and rather noisy) format used by most RL setups, defined by the OpenAI gym library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeEnv:\n",
    "\n",
    "\tWIN_MASKS = (0, 1, 2), (3, 4, 5), (6, 7, 8), (0, 3, 6), (1, 4, 7), (2, 5, 8), (0, 4, 8), (2, 4, 6)\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself.state = torch.zeros(9, device=device, dtype=int)  # first row is (0, 1, 2), first column (0, 3, 6)\n",
    "\t\tself.forbidden = torch.full((9,), False, device=device)\n",
    "\t\tself.player = 1\n",
    "\n",
    "\tdef step(self, action):\n",
    "\t\tassert not self.forbidden[action], f'Invalid action: {action} with state {self.state}'\n",
    "\t\tself.state[action] = self.player\n",
    "\t\tself.forbidden[action] = True\n",
    "\t\tfor a, b, c in TicTacToeEnv.WIN_MASKS:\n",
    "\t\t\tif self.state[a] == self.state[b] == self.state[c] == self.player:\n",
    "\t\t\t\tself.reward = 1\n",
    "\t\t\t\tself.done = 1\n",
    "\t\t\t\tbreak\n",
    "\t\telse:\n",
    "\t\t\tself.reward = 0\n",
    "\t\t\tself.done = self.forbidden.all()\n",
    "\t\tself.player = 1 if self.player == 2 else 2\n",
    "\n",
    "\tdef render(self):\n",
    "\t\tfor row in range(3):\n",
    "\t\t\tprint(''.join('·XO'[self.state[row * 3 + col]] for col in range(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test the environment with two random agents. We will define a random agent that will return a random probability distribution over the actions. Then, we will define a method to run a full episode while keeping track of the state (observations), actions and rewards. This method will have a parameter epsilon that will smooth the action probabilities, allowing the agent to explore the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "···\n",
      "···\n",
      "···\n",
      "\n",
      "Turn 0, player 0:\n",
      "X··\n",
      "···\n",
      "···\n",
      "Reward: 0, Done: False\n",
      "\n",
      "Turn 1, player 1:\n",
      "X··\n",
      "···\n",
      "··O\n",
      "Reward: 0, Done: False\n",
      "\n",
      "Turn 2, player 0:\n",
      "X··\n",
      "···\n",
      "X·O\n",
      "Reward: 0, Done: False\n",
      "\n",
      "Turn 3, player 1:\n",
      "X··\n",
      "·O·\n",
      "X·O\n",
      "Reward: 0, Done: False\n",
      "\n",
      "Turn 4, player 0:\n",
      "X··\n",
      "XO·\n",
      "X·O\n",
      "Reward: 1, Done: 1\n"
     ]
    }
   ],
   "source": [
    "class RandomAgent:\n",
    "\tdef __call__(self, _):\n",
    "\t\treturn torch.randn(9, device=device)\n",
    "\n",
    "def play_episode(env, players, epsilon=0, render=False):\n",
    "\tobservations, rewards, actions = [], [], []\n",
    "\tenv.reset()\n",
    "\tif render: env.render()\n",
    "\tturn = 0\n",
    "\twhile True:\n",
    "\t\tplayer = turn % len(players)\n",
    "\t\tif render: print(f'\\nTurn {turn}, player {player}:')\n",
    "\t\tobservations.append(env.state.clone())\n",
    "\t\tlogits = players[player](env.state)\n",
    "\t\tprobs = (logits.softmax(-1) * (1 - epsilon) + epsilon)  # if epsilon=1, probs are uniform\n",
    "\t\tprobs = torch.masked_fill(probs, env.forbidden, 0)  # don't sample forbidden actions\n",
    "\t\taction = torch.multinomial(probs, 1).item()\n",
    "\t\tactions.append(action)\n",
    "\t\tenv.step(action)\n",
    "\t\trewards.append(env.reward)\n",
    "\t\tif render:\n",
    "\t\t\tenv.render()\n",
    "\t\t\tprint(f'Reward: {env.reward}, Done: {env.done}')\n",
    "\t\tif env.done: break\n",
    "\t\tturn += 1\n",
    "\treturn observations, rewards, actions\n",
    "\n",
    "play_episode(TicTacToeEnv(), (RandomAgent(), RandomAgent()), render=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a way to collect experiences to train on them. Now let's define our player network as a very simple transformer with 128 hidden channels, 4 blocks with 4 attention heads each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor network has 794,753 parameters.\n"
     ]
    }
   ],
   "source": [
    "class Player(torch.nn.Module):\n",
    "\tdef __init__(self, hidden_size=128, num_blocks=4, num_heads=4):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.embedding = BoardEmbeding(hidden_size)\n",
    "\t\tself.hidden = torch.nn.Sequential(*[\n",
    "\t\t\tTransformerBlock(hidden_size, num_heads)\n",
    "\t\t\tfor _ in range(num_blocks)\n",
    "\t\t]).to(device)\n",
    "\t\tself.out = torch.nn.Linear(hidden_size, 1)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.embedding(x)\n",
    "\t\tx = self.hidden(x)\n",
    "\t\tx = self.out(x)\n",
    "\t\treturn x.squeeze(-1)\n",
    "\n",
    "class BoardEmbeding(torch.nn.Module):\n",
    "\tdef __init__(self, size):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.piece_embedding = torch.nn.Parameter(torch.randn(3, size, device=device))\n",
    "\t\tself.pos_embedding = torch.nn.Parameter(torch.randn(9, size, device=device))\n",
    "\t\tself.register_buffer('pos', torch.arange(9, device=device))\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.piece_embedding[x] + self.pos_embedding[self.pos]\n",
    "\t\treturn x\n",
    "\n",
    "class TransformerBlock(torch.nn.Module):\n",
    "\tdef __init__(self, hidden_size, num_heads, dropout=.1):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.norm1 = torch.nn.LayerNorm(hidden_size)\n",
    "\t\tself.attn = torch.nn.MultiheadAttention(hidden_size, num_heads, dropout)\n",
    "\t\tself.ff = torch.nn.Sequential(\n",
    "\t\t\ttorch.nn.Linear(hidden_size, 4 * hidden_size),\n",
    "\t\t\ttorch.nn.GELU(),\n",
    "\t\t\ttorch.nn.Linear(4 * hidden_size, hidden_size)\n",
    "\t\t)\n",
    "\t\tself.norm2 = torch.nn.LayerNorm(hidden_size)\n",
    "\t\tself.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.norm1(x)\n",
    "\t\tx = x + self.attn(x, x, x)[0]\n",
    "\t\tx = self.norm2(x)\n",
    "\t\tx = x + self.ff(x)\n",
    "\t\tx = self.dropout(x)\n",
    "\t\treturn x\n",
    "\n",
    "player = Player().to(device)\n",
    "print(f'Actor network has {sum(p.numel() for p in player.parameters()):,} parameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test it with our play episode method. Since agents are untrained, they won't be any better than the random agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions [2, 4, 7, 0, 3, 6, 1, 8]\n",
      "rewards [0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "observations, rewards, actions = play_episode(TicTacToeEnv(), (player, player))\n",
    "print('actions', actions)\n",
    "print('rewards', rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define the setup we will use to train the network. The most successful modern reinforcement learning algorithms are those defined as actor-critic. In an actor-critic setup, we are training two networks simultaneously: our agent, which tries to play the environment as good as possible, and our critic, which tries to estimate the value of the current policy. In this sense:\n",
    "\n",
    "* The actor receives the board state and returns the logits of all possible actions.\n",
    "* The critic is a function that receives the board state and returns a single number representing what is the expected return if you start in that state and then act according to the given policy.\n",
    "\n",
    "We already defined the actor, let's now define the critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critic network has 795,777 parameters.\n"
     ]
    }
   ],
   "source": [
    "class Critic(torch.nn.Module):\n",
    "\tdef __init__(self, hidden_size=128, num_blocks=4, num_heads=4):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.embedding = BoardEmbeding(hidden_size)\n",
    "\t\tself.hidden = torch.nn.Sequential(*[\n",
    "\t\t\tTransformerBlock(hidden_size, num_heads)\n",
    "\t\t\tfor _ in range(num_blocks)\n",
    "\t\t]).to(device)\n",
    "\t\tself.out = torch.nn.Linear(9 * hidden_size, 1)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.embedding(x)\n",
    "\t\tx = self.hidden(x)\n",
    "\t\tx = self.out(x.flatten(-2))\n",
    "\t\treturn x\n",
    "\n",
    "critic = Critic()\n",
    "print(f'Critic network has {sum(p.numel() for p in critic.parameters()):,} parameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are in a good position to implement some reinforcement learning algorithm. We will be implementing [Proximal Policy Optimization](https://arxiv.org/pdf/1707.06347) on the clipped variant (PPO-Clip), since it's one of the empirically best suited algorithms for reinforcement learning among many different contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(adv, epsilon=.2):\n",
    "\tif adv >= 0:\n",
    "\t\treturn (1 + epsilon) * adv\n",
    "\telse:\n",
    "\t\treturn (1 - epsilon) * adv\n",
    "\n",
    "def L(s, a, policy_k, policy):\n",
    "\tadv = advantage(s, a)\n",
    "\treturn min(adv * policy(s)[a] / policy_k(s)[a], g(adv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = BoardTransformer(4).to(device)\n",
    "print(f'Number of parameters in the policy net: {sum(p.numel() for p in policy.parameters()):,}')\n",
    "print(policy(new_board()))\n",
    "\n",
    "value = BoardTransformer(1).to(device)\n",
    "print(f'Number of parameters in the value net: {sum(p.numel() for p in value.parameters()):,}')\n",
    "print(value(new_board()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1      2      4      2\n",
      "    8     32     64      4\n",
      "    4     16     32      2\n",
      "    1      2      2      2\n"
     ]
    }
   ],
   "source": [
    "history = play_episode(policy)\n",
    "display_board(history['states'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\tdef __init__(self, maxlen):\n",
    "\t\tself.states = torch.empty((maxlen, 16, 4, 4), device=device)\n",
    "\t\tself.actions = torch.empty((maxlen,), dtype=torch.long, device=device)\n",
    "\t\tself.rewards = torch.empty((maxlen,), device=device)\n",
    "\t\tself.logits = torch.empty((maxlen, 4), device=device)\n",
    "\t\tself.append_idx = 0\n",
    "\t\tself.cycled = False\n",
    "\t\n",
    "\tdef append(self, state, action, reward, logits):\n",
    "\t\tself.states[self.append_idx] = state\n",
    "\t\tself.actions[self.append_idx] = action\n",
    "\t\tself.rewards[self.append_idx] = reward\n",
    "\t\tself.logits[self.append_idx] = logits\n",
    "\t\tself.append_idx += 1\n",
    "\t\tif self.append_idx == len(self.states):\n",
    "\t\t\tself.append_idx = 0\n",
    "\t\t\tself.cycled = True\n",
    "\t\n",
    "\tdef sample(self, batch_size):\n",
    "\t\tix = torch.randint(0, len(self.states) if self.cycled else self.append_idx, (batch_size,))\n",
    "\t\treturn self.states[ix], self.actions[ix], self.rewards[ix], self.logits[ix]\n",
    "\n",
    "buffer = ReplayBuffer(100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m \t\t\tbuffer\u001b[38;5;241m.\u001b[39mappend(state, action, reward, logits)\n\u001b[0;32m     32\u001b[0m \t\tupdate_policy(policy_net, value_net, buffer, optimizer_policy, optimizer_value)\n\u001b[1;32m---> 34\u001b[0m \u001b[43mtrain_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 32\u001b[0m, in \u001b[0;36mtrain_policy\u001b[1;34m(policy_net, value_net, buffer, num_episodes, lr)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state, action, reward, logits \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(episode_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m'\u001b[39m], episode_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m'\u001b[39m], episode_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrewards\u001b[39m\u001b[38;5;124m'\u001b[39m], episode_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m     31\u001b[0m \tbuffer\u001b[38;5;241m.\u001b[39mappend(state, action, reward, logits)\n\u001b[1;32m---> 32\u001b[0m \u001b[43mupdate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m, in \u001b[0;36mupdate_policy\u001b[1;34m(policy_net, value_net, buffer, optimizer_policy, optimizer_value, batch_size, epochs, gamma, clip)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      7\u001b[0m \tstates, actions, rewards, logits \u001b[38;5;241m=\u001b[39m buffer\u001b[38;5;241m.\u001b[39msample(batch_size)\n\u001b[1;32m----> 8\u001b[0m \tadvantages \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_advantages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \tlog_probs_old \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlog()\n\u001b[0;32m     10\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m state, action, reward, log_prob_old \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(states, actions, rewards, log_probs_old):\n",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m, in \u001b[0;36mcompute_advantages\u001b[1;34m(states, rewards, value_net, gamma)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_advantages\u001b[39m(states, rewards, value_net, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m \tvalues \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m rewards \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m values[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m-\u001b[39m values[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\juanc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\juanc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m, in \u001b[0;36mBoardTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 9\u001b[0m \tx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \tx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden(x)\n\u001b[0;32m     11\u001b[0m \tx \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\juanc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\juanc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m, in \u001b[0;36mBoardEmbeding.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m \texp_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \tx_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_x_embedding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_x)\n\u001b[0;32m     13\u001b[0m \ty_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_y_embedding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_y)\n",
      "File \u001b[1;32mc:\\Users\\juanc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\juanc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\juanc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\juanc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "def compute_advantages(states, rewards, value_net, gamma=0.99):\n",
    "\tvalues = value_net(states)\n",
    "\treturn rewards + gamma * values[1:] - values[:-1]\n",
    "\n",
    "def update_policy(policy_net, value_net, buffer, optimizer_policy, optimizer_value, batch_size=64, epochs=4, gamma=0.99, clip=0.2):\n",
    "\tfor _ in range(epochs):\n",
    "\t\tstates, actions, rewards, logits = buffer.sample(batch_size)\n",
    "\t\tadvantages = compute_advantages(states, rewards, value_net, gamma)\n",
    "\t\tlog_probs_old = logits.gather(1, actions.unsqueeze(-1)).squeeze(-1).log()\n",
    "\t\tfor state, action, reward, log_prob_old in zip(states, actions, rewards, log_probs_old):\n",
    "\t\t\toptimizer_policy.zero_grad()\n",
    "\t\t\toptimizer_value.zero_grad()\n",
    "\t\t\tlogits_new = policy_net(state.unsqueeze(0)).squeeze(0)\n",
    "\t\t\tlog_probs_new = logits_new.gather(0, action).log()\n",
    "\t\t\tratio = (log_probs_new - log_prob_old).exp()\n",
    "\t\t\tsurr1 = ratio * reward\n",
    "\t\t\tsurr2 = torch.clamp(ratio, 1.0 - clip, 1.0 + clip) * reward\n",
    "\t\t\tpolicy_loss = -torch.min(surr1, surr2).mean()\n",
    "\t\t\tpolicy_loss.backward()\n",
    "\t\t\toptimizer_policy.step()\n",
    "\t\t\tvalue_loss = ((value_net(state.unsqueeze(0)) - reward) ** 2).mean()\n",
    "\t\t\tvalue_loss.backward()\n",
    "\t\t\toptimizer_value.step()\n",
    "\n",
    "def train_policy(policy_net, value_net, buffer, num_episodes, lr=1e-4):\n",
    "\toptimizer_policy = torch.optim.Adam(policy.parameters(), lr=lr)\n",
    "\toptimizer_value = torch.optim.Adam(value.parameters(), lr=lr)\n",
    "\tfor _ in range(num_episodes):\n",
    "\t\tepisode_data = play_episode(policy_net)\n",
    "\t\tfor state, action, reward, logits in zip(episode_data['states'], episode_data['actions'], episode_data['rewards'], episode_data['logits']):\n",
    "\t\t\tbuffer.append(state, action, reward, logits)\n",
    "\t\tupdate_policy(policy_net, value_net, buffer, optimizer_policy, optimizer_value)\n",
    "\n",
    "train_policy(policy, value, buffer, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
