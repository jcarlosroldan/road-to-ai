{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `015` Residual connections\n",
    "\n",
    "Requirements: 008 Batch normalization, 011 Convolutional Neural Networks\n",
    "\n",
    "⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️ WIP\n",
    "\n",
    "Residual connections are a technique proposed by [He et al. 2015](https://arxiv.org/pdf/1512.03385) to train deep neural networks without the vanishing gradient problem. The idea is to add the input of a layer to its output. For instance, if we have a linear layer $L$ that receives an input $x$, instead of passing the signal $L(x)$ to the next layer, we pass $L(x) + x$. This simple change has a few implications:\n",
    "* Now $L$ is not a layer that just transforms $x$ into something else, but one that finds a \"patch\" to be applied to $x$.\n",
    "* If we stack many of these operations, the gradients won't become small, because $x$ is passed on to the next layer all the time.\n",
    "* If we see if the other way around, as $x + L(x)$, we can think of the network as a main branch that is $x$, that keeps flowing through the network, with forks that apply patches to it. This is the intuition behind the name \"residual\".\n",
    "\n",
    "Let's compare its impact over a deep network with and without residual connections. We will use the CIFAR-10 dataset and a simple convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: torch.Size([60000, 28, 28]), test: torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "train = torchvision.datasets.MNIST(root='data', download=True)\n",
    "test = torchvision.datasets.MNIST(root='data', download=True, train=False)\n",
    "x = train.data.float()\n",
    "xt = test.data.float()\n",
    "y = torch.nn.functional.one_hot(train.targets).float()\n",
    "yt = torch.nn.functional.one_hot(test.targets).float()\n",
    "print(f'Train: {x.shape}, test: {xt.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 491722 parameters in the model\n"
     ]
    }
   ],
   "source": [
    "class DigitRecognizer(torch.nn.Module):\n",
    "\tdef __init__(self, use_residual=False):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.use_residual = use_residual\n",
    "\t\tself.conv1_1 = torch.nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "\t\tself.conv1_2 = torch.nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "\t\tself.conv1_3 = torch.nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "\t\tself.pool1 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "\t\tself.conv2_1 = torch.nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "\t\tself.conv2_2 = torch.nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\t\tself.conv2_3 = torch.nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\t\tself.pool2 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "\t\tself.conv3_1 = torch.nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "\t\tself.conv3_2 = torch.nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\t\tself.conv3_3 = torch.nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\t\tself.pool3 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "\t\tself.fc = torch.nn.Linear(128 * 3 * 3, 10)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = x.unsqueeze(1)\n",
    "\t\tresidual = x\n",
    "\t\tx = self.conv1_1(x).relu()\n",
    "\t\tx = self.conv1_2(x).relu()\n",
    "\t\tx = self.conv1_3(x).relu()\n",
    "\t\tif self.use_residual: x += residual\n",
    "\t\tx = self.pool1(x)\n",
    "\t\tx = self.conv2_1(x).relu()\n",
    "\t\tresidual = x\n",
    "\t\tx = self.conv2_2(x).relu()\n",
    "\t\tx = self.conv2_3(x).relu()\n",
    "\t\tif self.use_residual: x += residual\n",
    "\t\tx = self.pool2(x)\n",
    "\t\tx = self.conv3_1(x).relu()\n",
    "\t\tresidual = x\n",
    "\t\tx = self.conv3_2(x).relu()\n",
    "\t\tx = self.conv3_3(x).relu()\n",
    "\t\tif self.use_residual: x += residual\n",
    "\t\tx = self.pool3(x)\n",
    "\t\tx = x.view(-1, 128 * 3 * 3)\n",
    "\t\tx = self.fc(x)\n",
    "\t\treturn x\n",
    "\n",
    "model = DigitRecognizer(use_residual=False)\n",
    "model_with_residual = DigitRecognizer(use_residual=True)\n",
    "print(f'There are {sum(p.numel() for p in model.parameters())} parameters in the model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, lr=0.01, num_iterations=5000, batch_size=32, print_each=500):\n",
    "\tmodel.train()\n",
    "\toptimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\tstart = time()\n",
    "\tfor i in range(num_iterations):\n",
    "\t\tix = torch.randint(0, x.shape[0], (batch_size,))\n",
    "\t\txb, yb = x[ix], y[ix]\n",
    "\t\tlogits = model(xb)\n",
    "\t\tloss = torch.nn.functional.cross_entropy(logits, yb.argmax(dim=1))\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\tif i % print_each == 0:\n",
    "\t\t\tremaining = (time() - start) * (num_iterations - i) / (i + 1)\n",
    "\t\t\tprint(f'Iteration {i:5}, loss: {loss.item():.6f}, remaining: {remaining//60:2.0f}m{remaining%60:2.0f}s')\n",
    "\tmodel.eval()\n",
    "\taccuracy = (model(xt).argmax(dim=1) == yt.argmax(dim=1)).float().mean().item()\n",
    "\tprint(f'Accuracy on the test set: {100 * accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model without residual connections\n",
      "Iteration     0, loss: 2.298797, remaining: 10m15s\n",
      "Iteration   500, loss: 0.121160, remaining:  4m23s\n",
      "Iteration  1000, loss: 0.002111, remaining:  3m54s\n",
      "Iteration  1500, loss: 0.069511, remaining:  3m26s\n",
      "Iteration  2000, loss: 0.001473, remaining:  2m57s\n",
      "Iteration  2500, loss: 0.022585, remaining:  2m27s\n",
      "Iteration  3000, loss: 0.020457, remaining:  1m58s\n",
      "Iteration  3500, loss: 0.000480, remaining:  1m28s\n",
      "Iteration  4000, loss: 0.066322, remaining:  0m59s\n",
      "Iteration  4500, loss: 0.002706, remaining:  0m29s\n",
      "Accuracy on the test set: 98.93%\n"
     ]
    }
   ],
   "source": [
    "print('Training the model without residual connections')\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model with residual connections\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [32, 128, 7, 7]], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining the model with residual connections\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_with_residual\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, lr, num_iterations, batch_size, print_each)\u001b[0m\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(logits, yb\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 11\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m print_each \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\juanc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\juanc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\juanc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [32, 128, 7, 7]], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "print('Training the model with residual connections')\n",
    "train(model_with_residual)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
