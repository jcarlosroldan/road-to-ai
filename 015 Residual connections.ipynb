{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `014` Residual connections\n",
    "\n",
    "Requirements: 008 Batch normalization, 011 Convolutional Neural Networks\n",
    "\n",
    "Residual connections are a technique proposed by [He et al. 2015](https://arxiv.org/pdf/1512.03385) to train deep neural networks without the vanishing gradient problem. The idea is to add the input of a layer to its output. For instance, if we have a linear layer $L$ that receives an input $x$, instead of passing the signal $L(x)$ to the next layer, we pass $L(x) + x$. This simple change has a few implications:\n",
    "* Now $L$ is not a layer that just transforms $x$ into something else, but one that finds a \"patch\" to be applied to $x$.\n",
    "* If we stack many of these operations, the gradients won't become small, because $x$ is passed on to the next layer all the time.\n",
    "* If we see if the other way around, as $x + L(x)$, we can think of the network as a main branch that is $x$, that keeps flowing through the network, with forks that apply patches to it. This is the intuition behind the name \"residual\".\n",
    "\n",
    "Let's compare its impact over a deep network with and without residual connections. We will use the CIFAR-10 dataset as in the CNN notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from time import time\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: torch.Size([60000, 28, 28]), test: torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "train = torchvision.datasets.MNIST(root='data', download=True)\n",
    "test = torchvision.datasets.MNIST(root='data', download=True, train=False)\n",
    "x = train.data.float()\n",
    "xt = test.data.float()\n",
    "y = torch.nn.functional.one_hot(train.targets).float()\n",
    "yt = torch.nn.functional.one_hot(test.targets).float()\n",
    "print(f'Train: {x.shape}, test: {xt.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define a CNN like the one in the previous notebook but with many more layers, so that we can create a need for residual layers by making the gradients vanish too fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 879242 parameters in the model\n"
     ]
    }
   ],
   "source": [
    "class DigitRecognizer(torch.nn.Module):\n",
    "\tdef __init__(self, use_residual=False):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.use_residual = use_residual\n",
    "\t\tself.conv1_1 = torch.nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "\t\tself.conv1_2 = torch.nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "\t\tself.conv1_3 = torch.nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "\t\tself.conv1_4 = torch.nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "\t\tself.conv1_5 = torch.nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "\t\tself.pool1 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "\t\tself.conv2_1 = torch.nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "\t\tself.conv2_2 = torch.nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\t\tself.conv2_3 = torch.nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\t\tself.conv2_4 = torch.nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\t\tself.conv2_5 = torch.nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\t\tself.pool2 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "\t\tself.conv3_1 = torch.nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "\t\tself.conv3_2 = torch.nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\t\tself.conv3_3 = torch.nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\t\tself.conv3_4 = torch.nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\t\tself.conv3_5 = torch.nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\t\tself.pool3 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "\t\tself.fc = torch.nn.Linear(128 * 3 * 3, 10)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = x.unsqueeze(1)\n",
    "\t\tresidual = x\n",
    "\t\tx = self.conv1_1(x).tanh()\n",
    "\t\tx = self.conv1_2(x).tanh()\n",
    "\t\tx = self.conv1_3(x).tanh()\n",
    "\t\tx = self.conv1_4(x).tanh()\n",
    "\t\tx = self.conv1_5(x).tanh()\n",
    "\t\tif self.use_residual: x = x + residual\n",
    "\t\tx = self.pool1(x)\n",
    "\t\tx = self.conv2_1(x).tanh()\n",
    "\t\tresidual = x\n",
    "\t\tx = self.conv2_2(x).tanh()\n",
    "\t\tx = self.conv2_3(x).tanh()\n",
    "\t\tx = self.conv2_4(x).tanh()\n",
    "\t\tx = self.conv2_5(x).tanh()\n",
    "\t\tif self.use_residual: x = x + residual\n",
    "\t\tx = self.pool2(x)\n",
    "\t\tx = self.conv3_1(x).tanh()\n",
    "\t\tresidual = x\n",
    "\t\tx = self.conv3_2(x).tanh()\n",
    "\t\tx = self.conv3_3(x).tanh()\n",
    "\t\tx = self.conv3_4(x).tanh()\n",
    "\t\tx = self.conv3_5(x).tanh()\n",
    "\t\tif self.use_residual: x = x + residual\n",
    "\t\tx = self.pool3(x)\n",
    "\t\tx = x.view(-1, 128 * 3 * 3)\n",
    "\t\tx = self.fc(x)\n",
    "\t\treturn x\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "model = DigitRecognizer(use_residual=False)\n",
    "torch.manual_seed(1234)\n",
    "model_with_residual = DigitRecognizer(use_residual=True)\n",
    "print(f'There are {sum(p.numel() for p in model.parameters())} parameters in the model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the residual connections are not any special kind of layers, but rather an operation performed at the forward pass. In the previous architecture:\n",
    "* We define three convolutional blocks that compress the image more on each of them.\n",
    "* Each block has 5 convolutional layers followed by a max pooling, so that each block processes images half the size of the previous one.\n",
    "* We have a total of 19 layers, making a network that starts being big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, lr=0.01, num_iterations=2000, batch_size=32, print_each=200):\n",
    "\ttorch.manual_seed(1234)\n",
    "\tmodel.train()\n",
    "\toptimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\tstart = time()\n",
    "\tlosses = []\n",
    "\tfor i in range(num_iterations):\n",
    "\t\tix = torch.randint(0, x.shape[0], (batch_size,))\n",
    "\t\txb, yb = x[ix], y[ix]\n",
    "\t\tlogits = model(xb)\n",
    "\t\tloss = torch.nn.functional.cross_entropy(logits, yb.argmax(dim=1))\n",
    "\t\tlosses.append(loss)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\tif i % print_each == 0 and i > 0 or i == 20:\n",
    "\t\t\tremaining = (time() - start) * (num_iterations - i) / (i + 1)\n",
    "\t\t\tprint(f'Iteration {i:5}, loss: {loss.item():.6f}, remaining: {remaining//60:2.0f}m{remaining%60:2.0f}s')\n",
    "\tmodel.eval()\n",
    "\taccuracy = (model(xt).argmax(dim=1) == yt.argmax(dim=1)).float().mean().item()\n",
    "\tprint(f'Accuracy on the test set: {100 * accuracy:.2f}%')\n",
    "\treturn losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model without residual connections\n",
      "Iteration    20, loss: 2.305718, remaining:  7m30s\n",
      "Iteration   200, loss: 2.287104, remaining:  6m31s\n",
      "Iteration   400, loss: 2.304785, remaining:  5m46s\n",
      "Iteration   600, loss: 2.302989, remaining:  5m 8s\n",
      "Iteration   800, loss: 2.304319, remaining:  4m26s\n",
      "Iteration  1000, loss: 2.289454, remaining:  3m42s\n",
      "Iteration  1200, loss: 2.306574, remaining:  2m58s\n",
      "Iteration  1400, loss: 2.292930, remaining:  2m20s\n",
      "Iteration  1600, loss: 2.307819, remaining:  1m36s\n",
      "Iteration  1800, loss: 2.304099, remaining:  0m47s\n",
      "Accuracy on the test set: 11.35%\n"
     ]
    }
   ],
   "source": [
    "print('Training the model without residual connections')\n",
    "losses = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model with residual connections\n",
      "Iteration    20, loss: 2.156486, remaining:  7m28s\n",
      "Iteration   200, loss: 0.602912, remaining:  6m40s\n",
      "Iteration   400, loss: 0.399495, remaining:  5m37s\n",
      "Iteration   600, loss: 0.372218, remaining:  4m47s\n",
      "Iteration   800, loss: 0.497465, remaining:  4m 2s\n",
      "Iteration  1000, loss: 0.195034, remaining:  3m21s\n",
      "Iteration  1200, loss: 0.078198, remaining:  2m40s\n",
      "Iteration  1400, loss: 0.277008, remaining:  1m60s\n",
      "Iteration  1600, loss: 0.402492, remaining:  1m20s\n",
      "Iteration  1800, loss: 0.197989, remaining:  0m40s\n",
      "Accuracy on the test set: 95.55%\n"
     ]
    }
   ],
   "source": [
    "print('Training the model with residual connections')\n",
    "losses_with_residual = train(model_with_residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by adding residual connections the accuracy goes really higher, and we are able to accurately train models with many layers. This opens the door for the design of arbitrarily large neural networks, in which the amount of layers we can add is no longer our bottleneck.\n",
    "\n",
    "This and other improvements that allowed for the creation of extremely deep neural networks with hundreds of layers, changed the paradigm of artificial intelligence. Now, the biggest bottleneck is just the hardware, which puts big players with resources to train really large networks in a priviledged position. This is a sad reality, as it closes the gates for small companies to push the frontiers of the field, at least in the current state. But hey, at least we get to have really smart models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
