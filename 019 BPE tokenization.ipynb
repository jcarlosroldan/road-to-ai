{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `019` Byte-pair encoding tokenization\n",
    "\n",
    "Requirements: 016 Transformers\n",
    "\n",
    "Even though transformers are powerful and severely speed up the training process thanks to parallelization, they are still limited by the maximum sequence length. Because the attention matrix is a square matrix of size `sequence_length x sequence_length`, the memory complexity of the model is $O(L^2)$, where $L$ is the sequence length. This means that the model can only handle sequences of a certain length.\n",
    "\n",
    "One of the most typical ways to handle this limitation is by adjusting how the input is tokenized. So far, we have been using a simple character-level tokenization, in which a sentence like \"I like trains\" would have $12$ tokens, and our total vocabulary was the set of all English characters and digits plus some special tokens, which in total is $\\approx 70$ tokens.\n",
    "\n",
    "If instead, we decided to use a word-level tokenization, the sentence \"I like trains\" would have $3$ tokens, and our vocabulary would be the set of all English words, which is $\\approx 200,000$ tokens. This would allow us to handle much longer sequences, but would create a huge embedding table, which would be very slow to train, and since many words are not very frequent, the embeddings would not be very good.\n",
    "\n",
    "This is why most modern models use a subword-level tokenization, which is a middle ground between character-level and word-level tokenization. The most popular subword tokenization algorithm is Byte-pair encoding (BPE). BPE, like most tokenization algorithms starts by converting the input text into a list of bytes, typically using UTF-8 encoding because it's [the best we got](https://www.reedbeta.com/blog/programmers-intro-to-unicode/). UTF-8 is a variable-length encoding, which means that each character can be represented by a different number of bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character a is represented by bytes [97]\n",
      "Character á is represented by bytes [195, 161]\n",
      "Character 🤯 is represented by bytes [240, 159, 164, 175]\n"
     ]
    }
   ],
   "source": [
    "characters = 'aá🤯'\n",
    "for c in characters:\n",
    "\tprint(f'Character {c} is represented by bytes {list(c.encode(\"utf-8\"))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have converted our text into a sequence of numbers $[0, 255]$ representing the bytes, the BPE algorithm is very simple: it just scans all pairs of elements keeping track of the most common pair of elements and merging them into a new element. This process is repeated as many times as necessary.\n",
    "\n",
    "Let's illustrate it over the dataset of Spanish novels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install regex\n",
    "from regex import compile as regex_compile\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, there are 19,979,810 UTF-8 bytes in the file.\n"
     ]
    }
   ],
   "source": [
    "with open('custom-data/spanish-novels.txt', encoding='utf-8') as fp:\n",
    "\ttext = fp.read()\n",
    "\n",
    "print(f'In total, there are {len(text.encode(\"utf-8\")):,} UTF-8 bytes in the file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a method that, given a sequence of elements, a pair to look for, and a new element, replaces all occurrences of the pair with the new element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H', 'o', 'w', 'X', 'r', 'e', ' ', 'y', 'o', 'u', '?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge(sequence, pair, replacement):\n",
    "\tres = []\n",
    "\ti = 0\n",
    "\twhile i < len(sequence):\n",
    "\t\tif i < len(sequence) - 1 and sequence[i] == pair[0] and sequence[i + 1] == pair[1]:\n",
    "\t\t\tres.append(replacement)\n",
    "\t\t\ti += 2\n",
    "\t\telse:\n",
    "\t\t\tres.append(sequence[i])\n",
    "\t\t\ti += 1\n",
    "\treturn res\n",
    "\n",
    "merge('How are you?', (' ', 'a'), 'X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before continuing with the tokenization method, there is a constraint we want to impose over the output tokens: we don't want tokens to mix characters from different categories (as in text, digits, whitespace of others).\n",
    "\n",
    "The reason why is that if we have some very common token like \"for\" and we tokenize it early on, we'll find many occurrences of that same token followed by different stuff, such as \"for.\", \"for?\", or \"for!\". This will create many tokens that encode the same stuff, which is not very useful.\n",
    "\n",
    "We can use the `regex` library, which is a more powerful version of the builtin `re` library that defines groups for unicode subsets. While the `\\w` in regex matches any letter or digit, the `\\p{L}` and `\\p{N}` match only letters and digits respectively. Plus, the `\\p{Z}` matches any kind of whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', \"'\", 's', ' see', ' how', ' this', ' w', '0', 'rks', '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = regex_compile(r'\\p{Z}?(?:\\p{L}+|\\p{N}+)|\\p{Z}+|.').findall\n",
    "split('Let\\'s see how this w0rks!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the required ingredients to implement our BPE tokenization method. Any tokenizer would have the following methods:\n",
    "* `train`: which uses some given dataset to learn the tokenization rules.\n",
    "* `encode`: which converts a given text into a list of tokens.\n",
    "* `decode`: which converts a list of tokens into a text.\n",
    "\n",
    "Let's start by implementing the `train` method of our BPE tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 merges done, 03:54:30 remaining, compression ratio 1:1.01\n",
      " 200 merges done, 02:30:31 remaining, compression ratio 1:1.98\n",
      " 400 merges done, 02:05:13 remaining, compression ratio 1:2.26\n",
      " 600 merges done, 01:45:46 remaining, compression ratio 1:2.43\n",
      " 800 merges done, 01:28:34 remaining, compression ratio 1:2.56\n",
      "1000 merges done, 01:12:31 remaining, compression ratio 1:2.66\n",
      "1200 merges done, 00:57:16 remaining, compression ratio 1:2.75\n",
      "1400 merges done, 00:42:28 remaining, compression ratio 1:2.82\n",
      "1600 merges done, 00:28:03 remaining, compression ratio 1:2.88\n",
      "1800 merges done, 00:13:56 remaining, compression ratio 1:2.94\n",
      "Final compression ratio 1:2.99\n"
     ]
    }
   ],
   "source": [
    "def train_tokenizer(text, max_merges=2000):\n",
    "\tpieces = split(text)\n",
    "\telems = [list(text.encode('utf-8')) for text in pieces]\n",
    "\tstart = time()\n",
    "\tnext_replacement = 256\n",
    "\tvocabulary = [[i] for i in range(next_replacement)]\n",
    "\tmerges = {}\n",
    "\tfor m in range(max_merges):\n",
    "\t\t# find the most frequent pair of elements across all pieces\n",
    "\t\tcount = {}\n",
    "\t\tfor piece in elems:\n",
    "\t\t\tfor e1, e2 in zip(piece[:-1], piece[1:]):\n",
    "\t\t\t\tcount[(e1, e2)] = count.get((e1, e2), 0) + 1\n",
    "\t\tpair, freq = max(count.items(), key=lambda x: x[1])\n",
    "\t\t# if the pair is not frequent enough, stop\n",
    "\t\tif freq == 1:\n",
    "\t\t\tprint('Stopping early, no pair found more than once.')\n",
    "\t\t\tbreak\n",
    "\t\t# merge the pair\n",
    "\t\tvocabulary.append(vocabulary[pair[0]] + vocabulary[pair[1]])\n",
    "\t\tmerges[pair] = next_replacement\n",
    "\t\tfor i, piece in enumerate(elems):\n",
    "\t\t\telems[i] = merge(piece, pair, next_replacement)\n",
    "\t\tnext_replacement += 1\n",
    "\t\t# print progress\n",
    "\t\tif m % (max_merges // 10) == 0:\n",
    "\t\t\tremaining = (time() - start) * (max_merges - m) / (m + 1)\n",
    "\t\t\tremaining = f'{remaining//3600:02.0f}:{remaining%3600//60:02.0f}:{remaining%60:02.0f}'\n",
    "\t\t\tprint(f'{m:4} merges done, {remaining} remaining, compression ratio 1:{len(text) / sum(map(len, elems)):.2f}')\n",
    "\treturn elems, vocabulary, merges\n",
    "\n",
    "compressed_text, vocabulary, merges = train_tokenizer(text)\n",
    "print(f'Final compression ratio 1:{len(text) / sum(map(len, compressed_text)):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train` method has gone over the dataset looking for common pair of elements, and outputs the following structures:\n",
    "* `compressed_text`: The text encoded using our tokenization rules, which can be used to compute the compression ratio or to find further merges.\n",
    "* `vocabulary`: A dictionary that maps each token to the set of bytes it represents.\n",
    "* `merges`: A dictinary that maps each pair of tokens to the new token that replaces them.\n",
    "\n",
    "Both the `vocabulary` and `merges` dictionaries required to implement the `encode` and `decode` methods. Let's implement them and see if they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: this is a test to see if it works °å^○ⁿ·\n",
      "Encoded text: [116, 104, 499, 385, 115, 265, 285, 271, 116, 380, 304, 101, 385, 102, 385, 116, 32, 119, 277, 107, 115, 32, 194, 176, 195, 165, 94, 226, 151, 139, 226, 129, 191, 194, 183]\n",
      "Decoded text: this is a test to see if it works °å^○ⁿ·\n"
     ]
    }
   ],
   "source": [
    "def encode(text, merges):\n",
    "\tres = list(text.encode('utf-8'))\n",
    "\tfor pair, replacement in merges.items():\n",
    "\t\tres = merge(res, pair, replacement)\n",
    "\treturn res\n",
    "\n",
    "def decode(elems, vocabulary):\n",
    "\tres = b''.join(bytes(vocabulary[e]) for e in elems)\n",
    "\treturn res.decode('utf-8')\n",
    "\n",
    "text = 'this is a test to see if it works °å^○ⁿ·'\n",
    "encoded = encode(text, merges)\n",
    "decoded = decode(encoded, vocabulary)\n",
    "print(f'Original text: {text}')\n",
    "print(f'Encoded text: {encoded}')\n",
    "print(f'Decoded text: {decoded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! We have implemented a simple BPE tokenizer and trained it to create 2000 new tokens from the dataset of Spanish novels.\n",
    "\n",
    "Note that generating a tokenizer is a slow process (it took us 4 hours in a M2 Macbook Pro) even on a tiny dataset like ours. Doing this at large scale requires figuring out more complex processes that parallelize the search for the most common pair of elements and way more computational resources. This is why most people use pre-trained tokenizers such as [OpenAI's tiktoken](https://github.com/openai/tiktoken) or Google's [SentencePiece](https://github.com/google/sentencepiece).\n",
    "\n",
    "The biggest problem with these tokenizers is that they are mostly trained over English text, which means that they are not very good at tokenizing text in other languages (including programming languages). Plus, tokenization is at the root of many AI problems such as:\n",
    "* Not being able to perform simple arithmetic like 123 + 456, because maybe the tokenizer splits 123 as \"12\" and \"3\".\n",
    "* Not being able to perform simple string operation like counting, spelling or reversing sentences.\n",
    "\n",
    "It is one of the most hated pieces of the current status of AI, and ideally having alternatives to attention mechanisms that can handle arbitrarily longer sequences and being able to use character-level tokenization would be a huge step forward. However, this is the best we got so far."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
