{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `019` Byte-pair encoding tokenization\n",
    "\n",
    "Requirements: 016 Transformers\n",
    "\n",
    "⚡⚡⚡WIP! DONT READ YET⚡⚡⚡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install regex\n",
    "from regex import compile as regex_compile\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, there are 19,979,810 UTF-8 bytes in the file.\n"
     ]
    }
   ],
   "source": [
    "with open('custom-data/spanish-novels.txt', encoding='utf-8') as fp:\n",
    "\ttext = fp.read()\n",
    "\n",
    "print(f'In total, there are {len(text.encode(\"utf-8\")):,} UTF-8 bytes in the file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the idea is simple we merge the most common pair over and over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H', 'o', 'w', 'X', 'r', 'e', ' ', 'y', 'o', 'u', '?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge(elems, pair, replacement):\n",
    "\tres = []\n",
    "\ti = 0\n",
    "\twhile i < len(elems):\n",
    "\t\tif i < len(elems) - 1 and elems[i] == pair[0] and elems[i + 1] == pair[1]:\n",
    "\t\t\tres.append(replacement)\n",
    "\t\t\ti += 2\n",
    "\t\telse:\n",
    "\t\t\tres.append(elems[i])\n",
    "\t\t\ti += 1\n",
    "\treturn res\n",
    "\n",
    "merge('How are you?', (' ', 'a'), 'X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before thinking of any tokenization method, there is a constraint we want to impose over the output tokens: we don't want tokens to contain letters from different categories such as text, digits, whitespace or others. The reason why is that, since BPE will perform merges based on the most common pairs, if a very common word like `for` is tokenized early on, we'll find many occurrences of that same word followed by different stuff, such as `for.`, `for?`, or `for!`.\n",
    "\n",
    "To avoid creating too many tokens that encode the same stuff, we will split into these categories using the `regex` library, a more powerful version of the builtin `re` library that defines groups for unicode subsets. While the `\\w` in regex matches any letter or digit, the `\\p{L}` and `\\p{N}` match only letters and digits respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', \"'\", 's', ' see', ' how', ' this', ' w', '0', 'rks', '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = regex_compile(r'\\p{Z}?(?:\\p{L}+|\\p{N}+)|\\p{Z}+|.').findall\n",
    "split('Let\\'s see how this w0rks!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 merges done, 01:59:16 remaining, compression ratio: 1.01\n"
     ]
    }
   ],
   "source": [
    "def train_tokenizer(text, max_merges=1000):\n",
    "\tpieces = split(text)\n",
    "\telems = [list(text.encode('utf-8')) for text in pieces]\n",
    "\tstart = time()\n",
    "\tnext_replacement = 256\n",
    "\tvocabulary = [[i] for i in range(next_replacement)]\n",
    "\tmerges = {}\n",
    "\tfor m in range(max_merges):\n",
    "\t\t# find the most frequent pair of elements across all pieces\n",
    "\t\tcount = {}\n",
    "\t\tfor piece in elems:\n",
    "\t\t\tfor e1, e2 in zip(piece[:-1], piece[1:]):\n",
    "\t\t\t\tcount[(e1, e2)] = count.get((e1, e2), 0) + 1\n",
    "\t\tpair, freq = max(count.items(), key=lambda x: x[1])\n",
    "\t\t# if the pair is not frequent enough, stop\n",
    "\t\tif freq == 1:\n",
    "\t\t\tprint('Stopping early, no pair found more than once.')\n",
    "\t\t\tbreak\n",
    "\t\t# merge the pair\n",
    "\t\tvocabulary.append(vocabulary[pair[0]] + vocabulary[pair[1]])\n",
    "\t\tmerges[pair] = next_replacement\n",
    "\t\tfor i, piece in enumerate(elems):\n",
    "\t\t\telems[i] = merge(piece, pair, next_replacement)\n",
    "\t\tnext_replacement += 1\n",
    "\t\t# print progress\n",
    "\t\tif m % (max_merges // 10) == 0:\n",
    "\t\t\tremaining = (time() - start) * (max_merges - m) / (m + 1)\n",
    "\t\t\tremaining = f'{remaining//3600:02.0f}:{remaining%3600//60:02.0f}:{remaining%60:02.0f}'\n",
    "\t\t\tprint(f'{m:4} merges done, {remaining} remaining, compression ratio: {len(text) / sum(map(len, elems)):.2f}')\n",
    "\treturn elems, vocabulary, merges\n",
    "\n",
    "compressed_text, vocabulary, merges = train_tokenizer(text)\n",
    "print(f'Final compression ratio: {len(text) / sum(map(len, compressed_text)):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: this is a test to see if it works °å^○ⁿ·\n",
      "Encoded text: [116, 104, 105, 115, 32, 105, 115, 259, 32, 270, 115, 116, 32, 291, 301, 101, 32, 105, 102, 32, 105, 116, 32, 119, 111, 114, 107, 115, 32, 194, 176, 195, 165, 94, 226, 151, 139, 226, 129, 191, 194, 183]\n",
      "Decoded text: this is a test to see if it works °å^○ⁿ·\n"
     ]
    }
   ],
   "source": [
    "def encode(text, merges):\n",
    "\tres = list(text.encode('utf-8'))\n",
    "\tfor pair, replacement in merges.items():\n",
    "\t\tres = merge(res, pair, replacement)\n",
    "\treturn res\n",
    "\n",
    "def decode(elems, vocabulary):\n",
    "\tres = b''.join(bytes(vocabulary[e]) for e in elems)\n",
    "\treturn res.decode('utf-8')\n",
    "\n",
    "text = 'this is a test to see if it works °å^○ⁿ·'\n",
    "encoded = encode(text, merges)\n",
    "decoded = decode(encoded, vocabulary)\n",
    "print(f'Original text: {text}')\n",
    "print(f'Encoded text: {encoded}')\n",
    "print(f'Decoded text: {decoded}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
