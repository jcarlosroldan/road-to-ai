{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `011` Convolutional Neural Networks\n",
    "\n",
    "Requirements: 007 Initialization, 008 Batch normalization\n",
    "\n",
    "With the MLP architectures we have seen so far, we take a set of input features, make linear combinations of them, and then apply a non-linear function to the result. This is very flexible, but often we want to take advantage of the structure of the input data. For example, in images, we know that pixels that are close to each other are more likely to be related than pixels that are far apart. Convolutional Neural Networks (CNNs) are a type of neural network that takes advantage of this structure.\n",
    "\n",
    "In the 50s, Hubel and Wiesel discovered that the area of the brain that processes visual information had areas that respond individually to small regions of the visual field. They also discovered that some neurons responded to edges, others to corners, and so on. This led to the idea that the brain processes visual information in a hierarchical way, with simple features being detected first, and then more complex features being built on top of them.\n",
    "\n",
    "CNNs are inspired by this idea. They have a series of layers, each of which is made up of a number of filters. Each filter is a small matrix that is applied to the input data. The output of the filter is a new matrix that is the result of applying the filter to the input data. The filters are learned during training, so the network can learn to detect different features in the input data.\n",
    "\n",
    "Let's consider the following image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from time import time\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAAAAABVicqIAAABB0lEQVR4Ae3W4Q6DIAwEYNz7v/NmAkQusB4qaWK9/XGkrqwft8WU9JKABN4jsE2N+jXvoj0+5scXFV02IaPaTjCo0cllkjib/JM8cRhwMmnUMDjXZass14lF5rppVbIGZHG4NAn+l5CVCxdELa2Jb5dil0m0CckTluNwNRFemt/sVbrH4dIk+EMgK3ERICyLCz3ISlwECMviQg+yEhcBwrK40IOsxEWAsCwu9CCr5ql+v3Ppg/3RWmdCTgHLcbiOCNQJ1yQM+sbh0iQ1JlNXFy6IWv1aN1Pc9XSZxGWTbrRCdlls1NBlkjibjAhrkvfriaMxOr2GayrQBlT+fByuJkl6K4EnCvwA3E8NmkuqrV8AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=100x100>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.meshgrid(torch.arange(-15, 15), torch.arange(-15, 15), indexing='ij')\n",
    "img = (img[0]**2 + img[1]**2 < 13**2).float()\n",
    "\n",
    "# display a 10-sized version of the image\n",
    "Image.fromarray(img.numpy().astype('uint8') * 255).resize((100, 100), Image.NEAREST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For detecting the borders of an image like this, we need to apply two filters: one for detecting vertical borders and another for detecting horizontal borders. For instance, this is the vertical border filter:\n",
    "\n",
    "```\n",
    "1  0 -1\n",
    "1  0 -1\n",
    "1  0 -1\n",
    "```\n",
    "\n",
    "Applying a filter means starting on the top-right corner, taking a crop of the image with the same size as the filter (this is called the **window**), multiplying the values of the filter by the values of the image, and summing the results. This sum is the value of the output matrix at the top-right corner. Then we move the window one pixel to the right and repeat the process. When we reach the end of the row, we move the window to the leftmost pixel of the next row and repeat the process. Sliding the window through the image in this way is called a **convolution**.\n",
    "\n",
    "Intuitively, if we are looking at a window like this one:\n",
    "\n",
    "```\n",
    "1 1 1\n",
    "2 2 1\n",
    "1 1 3\n",
    "```\n",
    "\n",
    "When doing the pairwise multiplication by the filter we'll obtain the following matrix:\n",
    "\n",
    "```\n",
    "1 0 -1\n",
    "2 0 -1\n",
    "1 0 -3\n",
    "```\n",
    "\n",
    "Since we are going to sum all elements, that means that:\n",
    "* We don't care about the middle column of the window\n",
    "* When we sum the first row we obtain a zero, because the first row of the original window is unchanged, i.e. it doesn't have a border.\n",
    "* When we sum the second row we obtain a 1, because there is some difference between the first and second row of the window.\n",
    "* When we sum the third row we obtain a -2, because there is some difference between the second and third row of the window. Note that a negative value means that the border has its darker side on the left.\n",
    "* When adding up all numbers we obtain -1, which is the value of the output matrix at the pixel in the center of the pixel.\n",
    "\n",
    "Let's run this convolution over the full image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCABkAGQBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AEopCQoJJAA5JNVbm9jjt2eKWJnGMDdnPNRWF/LdTsjqgAXPyg+orQoq7p9nHd+Z5jMNuMbT65ovLBoZgsCSupXJOM8/gKpkFSQQQRwQaSiiiobm5S1jDuGIJx8tYH9syXc1zFG7BFYqQyjoc0kELXEyxIQGbpnpWtYWEtrOzuyEFcfKT6itCirun3kdp5nmKx3YxtHpmti2uUuoy6BgAcfNT4dGgvZ3AjXecsSzEd//AK9c7c2z2sgRypJGflqGikJCgkkADkk1wfivXZEvZbW2LSlHU4jl7bfQfWtPw5pUjwrdzFl81Y5Crp1zyRk9etdMsEKMGSKNWHQhQDUlFFFPSaWMYSV1HXCsRW9o+trBNiVBxHjc0mMnj2rpNf8ADsTQT3FuElEdux3Rwg8gE4yK87ZWRirqVYdQRg0lUNWvobOwui7gOIHYAgnsfT6V57pFnLrviC4uQpMbw5BQgZxtHevS7SLyLOCE5/dxqvPsMVNRRRRRRXq+g6pBqmh3URcb5HeJQqkZyo9frXnniK1Nlr1zbkEFNvUgnlQe31rLrz/x7qP2fURbbpR5lp0U8clhzzWp4FstmiW13tj+dHXIHzffPX8q6yiiiiiiiug8L6j9n1Gztt0o8y7ThTxyVHPNHjf/AJG++/7Z/wDota5+vMPiP/yMNv8A9ei/+hvXX+Cf+RQsf+2n/oxq6CiiiiiiitDQf+Rh0z/r7i/9DFaHjf8A5G++/wC2f/ota5+vMPiP/wAjDb/9ei/+hvXX+Cf+RQsf+2n/AKMaugooooooorQ0H/kYdM/6+4v/AEMVoeN/+Rvvv+2f/ota5+vPfH2myXOprdLu2x2gzhMjhmPWtjwLeo+g2tmNu+NXY/Nz/rD2/Guqooooooord8M6bJc6nZXS7tsd3HnCZHDA9af43/5G++/7Z/8Aota5+szWtOS9sLokvvNu6gKQOx/xrhPD98+ha3PauFVI4SAWBY8lW7fWvSbaXz7WKYYxIgbj3Galoooooor1Hw1pKaZo1xPIZFaKRpQCQRwoPb6Vwvie6F74iurgEEPs6AgcIB3+lZFIQGBBAIPBBrzvxfpcg1Ga4tvKiLOgyvynGz2HtW94Z1QfZY7WVpXdEjjyTkZxj1rp6KKKKK3NG0aS4m3N5LK0e4BufT2rsvEWrxR2txDbiWIPbtwmFGSCM8GvMmZnYs7FmPUk5NJRUNzbJdRhHLAA5+Wuc/sxrS4uXjjmKs5JLDjAJ9qfBM1vMsqAFl6Z6VrWF/LdTsjqgAXPyg+orQoq7p9nHd+Z5jMNuMbT65rYtrZLWMohYgnPzVPb6jHZzMyyw7sbSGbp+tczc3L3UgdwoIGPlqGiiikZQ6MjDKsMEVV/s20/55f+PH/GpIbSC3cvEm1iMZyTU9FSw3M1vu8p9u7rwDU39pXf/Pb/AMdH+FVXYu7OxyzHJNJRRRRRRRRRRRRRRRX/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAAAAABVicqIAAAB8UlEQVR4Ae2XAW7DIAxFs2mn2c7T8+xAO892nv0vmxiWBBql/ZMsUJVCgnH/8yeiL5/L89vr81MsiyTJW1/Jjz1+35/1Ybe/95/GXYmSPEmOauLFCK6dnpWmU5nMuM6Aqhl2/JwVF1HBJa4cg4Ot7pjcUH+nb3yWB5dESbXjUZCg2S9Gbd0S5ZXxwFgJcyVKJEkqXFBIC99Gxm1ZrSMHhHA2H9lAokSSpOACKHxojTO+MhpxBSSE02eNVSVK8iQpNTHLxfs0OJ/soRhcBaUJF+fCBYXu3yv2dazmYiL7WkHnwuU2eIC1DBDxN4vlwSVRwh1PhmgP8K8txBrfrGtXiZKZpEY+7E9cQ0T1hImrpjHsT1xDRPWEiaumMexPXENE9YSJq6Yx7PNIxLMQjkX4POZU1BxR+Qvy1IS4KA+s/O8L9V1rG+h5cEmUFAuDo7n4qo1RYNbY/71ZcSVKJEmIiw0SzcVXcbl/sVg0iRJJkoILKs1gl96TZi06K6m7pDUBRm5UNKPpTrRbd10ZjiiEN/ZlrESJJEmxsGmlNsPlku+i5pwZvL43bOBXiRJJkgoXtG3o9Kk1nIgG05utng7XP9SEDK0um13reI+/9orhsyVKJElaCweMjZnjUfQ6hGISehIlM0nDfDSYuEaEmud5cP0C9Eo+lW9x76AAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=100x100>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter = torch.tensor([[1, 0, -1], [1, 0, -1], [1, 0, -1]])\n",
    "output = torch.zeros(img.shape)\n",
    "for x in range(0, img.shape[0] - 3):\n",
    "\tfor y in range(0, img.shape[1] - 3):\n",
    "\t\toutput[x, y] = (img[x:x+3, y:y+3] * filter).sum()\n",
    "output = 255 * (output - output.min()) / (output.max() - output.min())\n",
    "Image.fromarray(output.numpy().astype('uint8')).resize((100, 100), Image.NEAREST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like that, we have detected the vertical borders of the image. We can do the same thing with a filter that detects horizontal borders, but this time we'll let torch do the for loops for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 30])\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCABkAGQBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AEooqCS8tYf9bcwpg4+ZwOa5y98b2lrHuiSGc4J2rcjt+BrJ/wCFm/8AUI/8mf8A7Cj/AIWb/wBQj/yZ/wDsKP8AhZv/AFCP/Jn/AOwrbsPGVneSiOTyIcsoBa4HOfwFb0d5azf6q5hfJx8rg81PRRRTJZEhjMkhwo6nFc1rXi1dNciFomwwHzox6jPauTuNS1nWJHEVpbsjMZVKnGR+Le9X7LwA8kmLxLiNMjlZU6d/WtX/AIVxo/8Az833/faf/E0f8K40f/n5vv8AvtP/AImj/hXGj/8APzff99p/8TWNeeBp7RBPaRzSNGC+HlTGRyPSqlvqWs6PIgltLdUVhKxY5wPwb2rr9I8VJf8Ak+a8Q37s7Ubtn/CukVg6K6nKsMg0tFYuuy3O0wQSBNyA8gY6/T2rn4PDzardOL3ypRt3feZeRgdsdq6Wz8P6dZxoEtgHCBSRIx/mfatWiiikZQ6MjDKsMEVl3nh/TryNw9sC5QqCZGH8j71y8uhy6deFLRoo1j+6NxOMjnqPc102j3Fw5WGaQNsiHQDqMCteorklbWUgkEISCPpXOPI8hy7sx6ZY5rc0yNBZxuEUOcgtjk81cooooooqlqUUf2OWTy138fNjnqO9YiSPGco7KemVOK6is3VopJPJ8uNmxuztGcdKq6fBMl9GzxSKozklSB0NblFFFFFFFFc/c207XUpEMhBckEKfWovstx/zwl/74NdLRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRWno8iIZQzqpbaACcZ61f1BWexkVFLMcYAGT1Fc8ysjFXUqw6gjBpKKKKKKKVVZ2CopZj0AGTXS2wK2sIIIIQAg/Sj7Vb/894v++xXPWxC3UJJAAcEk/WukSRJBlHVh0ypzWHqcbi9kcowQ4AbHB4FUqKKKKKKu6bFJ9sik8ttnPzY46HvW48iRjLuqjpljiuWorc8Pz2qusNw3DOTjB/u+30rpJ9Eh1W2VbK3807t33yvAyO5HeuUvfD+o2k0m+12IJCo/eKfX3rKooopVUu6ooyzHAFall4f1G7mj2Wu9DIFP7xR6e9ddFpFvptmI7uDy2j+8N5OMnjofcVyms3NvLvigfO2U4GDwBn1rHopVZkYMjFWHQg4NdFoviUaagE0l0xCkfIc9Tnua7iy1zSdRhjV7JnYxhyZYkOTxz1681R1LwW1zDttYrGJtrDO3byenRawv+Fcax/z82P8A32//AMTR/wAK41j/AJ+bH/vt/wD4mj/hXGsf8/Nj/wB9v/8AE10Nh4SWwdp7qCykVCHwEycDk9RVq91zSdOhkVLJkYRlwYokGDzz168Vwmr+Ivt/neVJcjftxub0x7+1YBJYkkkk8kmkoooor0Xw5/x9/wDbRP5mu7oorzrxH/x6f9s3/kK4Siiiv//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAAAAABVicqIAAAByklEQVR4Ae2aYW7DMAiF3Wmn2c7T8+yU23n2XoKFcdPirS0/KEip4xhj8fGotqinr/Z8e3v+Ea3lOeTdw/UDh+/WOBzaB55+tsbhqr0CLo+TwKEbLqF2jC0PrpBMDiQMwtBsF60n0aF0vIU7BD2VJiSTkEMmXEPyHidAodENl2zskGWBDrCQTPIcMtYEXKld2BkXYa+blIY1YQyJIzFy4kKa7FbY31Dte/bPw515cIVk0iXMrwWxQ6x9cWHkfgmHgbOQTOqQhdqoSx5cm4RVv/fKtyNCHFVxSbhjWRvzqCskk03CjxKuqdAQNCSTOsTg9yaFyyNk1guXweFNCpdHyKwXLoPDmxQuj5BZL1wGhzfJg2v7405ediBrebvj5b+yrkHr/5MVXuqTR10hmWwShnBFcBweIWPVL6OFZFKHaBMs3BWuBUjqsvUJppSz9spdraI90lsuT006LgE4vKv4HzKwkhj6OrC+VlSeK3djTaQKYHrGVqIF18XSULfcgZE77Ma0Eh7gEIAI0iaP54MJJw40BKB2hziY5cEVksnp6g+WJtgEfsNEt1MxZENIJiGHjB1vaRCACBKt3CVqfXaXqb8vXErCl0huPfkFY3NACgnS2zoAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=100x100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter = torch.tensor([[1, 1, 1], [0, 0, 0], [-1, -1, -1]]).float()\n",
    "print(img.shape)  # torch.Size([30, 30])\n",
    "output = torch.conv2d(img[None, None, :, :], filter[None, None, :, :])\n",
    "output = 255 * (output - output.min()) / (output.max() - output.min())\n",
    "Image.fromarray(output[0, 0].numpy().astype('uint8')).resize((100, 100), Image.NEAREST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could stack the result of both filters to get a 3D matrix with two channels, one for each filter. This is called a **feature map**. We could then take the feature map and apply more convolutions, this time 3-dimensional, to detect more complex features. For instance, we could create a feature that represents the direction of the border, knowing that $atan2(\\text{vertical border}, \\text{horizontal border})$ will give us the direction of the border.\n",
    "\n",
    "This is the idea behind CNNs and its success on image processing. The only difference is that in practice the filters are not hand-crafted, but learned during training. There are a few hyperparameters involved in a convolutional layer:\n",
    "* The number of filters\n",
    "* The size of the filters: typically, filters are square matrices with odd dimensions, like 3x3 or 5x5.\n",
    "* The stride: how many pixels we move the window at each step. A stride of 1 means that we move the window one pixel at a time, a stride of 2 means that we move the window two pixels at a time, and so on.\n",
    "* Padding: what we do when the window reaches the edge of the image. We can either ignore the pixels that are outside the image, or we can pad the image with zeros so that the window can slide all the way to the edge. By default, PyTorch uses zero padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: torch.Size([60000, 28, 28]), test: torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "train = torchvision.datasets.MNIST(root='data', download=True)\n",
    "test = torchvision.datasets.MNIST(root='data', download=True, train=False)\n",
    "x = train.data.float()\n",
    "xt = test.data.float()\n",
    "y = torch.nn.functional.one_hot(train.targets).float()\n",
    "yt = torch.nn.functional.one_hot(test.targets).float()\n",
    "print(f'Train: {x.shape}, test: {xt.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last key ingredient before stacking a bunch of convolutional filters together is the pooling layer. In the end, our goal with convolutional layers is to detect features in the input data. Same as in the brain equivalent, we need to create a hierarchy of features, starting from simple features and building more complex features on top of them. The pooling layer takes a window of the input data and applies a function to it, like the maximum or the average. This reduces the size of the input data, which compressed the information and makes the network more robust to small changes in the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 103018 parameters in the model\n"
     ]
    }
   ],
   "source": [
    "class DigitRecognizer(torch.nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# take the input (B x 28 x 28), add a channel dimension (B x 1 x 28 x 28) and apply a 3x3 convolution with 8 filters\n",
    "\t\tself.conv1 = torch.nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "\t\t# apply a 2x2 max pooling -> B x 8 x 14 x 14\n",
    "\t\tself.pool1 = torch.nn.MaxPool2d(2)\n",
    "\t\t# apply a 3x3 convolution with 16 filters -> B x 16 x 14 x 14\n",
    "\t\tself.conv2 = torch.nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "\t\t# apply a 2x2 max pooling -> B x 16 x 7 x 7\n",
    "\t\tself.pool2 = torch.nn.MaxPool2d(2)\n",
    "\t\t# flatten the output -> B x 16*7*7 and apply a linear layer\n",
    "\t\tself.fc = torch.nn.Linear(16*7*7, 128)\n",
    "\t\t# apply a ReLU activation and a linear layer for the output -> B x 10\n",
    "\t\tself.output = torch.nn.Linear(128, 10)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = torch.relu(self.conv1(x.unsqueeze(1)))  # unsqueeze adds the missing channel dimension\n",
    "\t\tx = self.pool1(x)\n",
    "\t\tx = torch.relu(self.conv2(x))\n",
    "\t\tx = self.pool2(x)\n",
    "\t\tx = x.view(x.shape[0], -1)  # flatten all filters into a single vector\n",
    "\t\tx = torch.relu(self.fc(x))\n",
    "\t\tx = self.output(x)\n",
    "\t\treturn x\n",
    "\n",
    "model = DigitRecognizer()\n",
    "print(f'There are {sum(p.numel() for p in model.parameters())} parameters in the model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train this network and see what we got. In the batch normalization notebook we achieved ~89% accuracy with the best model we could find, having the same number of parameters as this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss: 8.946130752563477, remaining: 25s\n",
      "Iteration 500, loss: 0.19320261478424072, remaining: 16s\n",
      "Iteration 1000, loss: 0.027505548670887947, remaining: 14s\n",
      "Iteration 1500, loss: 0.23153482377529144, remaining: 12s\n",
      "Iteration 2000, loss: 0.042928993701934814, remaining: 10s\n",
      "Iteration 2500, loss: 0.02033296972513199, remaining: 9s\n",
      "Iteration 3000, loss: 0.005385107826441526, remaining: 7s\n",
      "Iteration 3500, loss: 0.004117920063436031, remaining: 5s\n",
      "Iteration 4000, loss: 0.025947317481040955, remaining: 3s\n",
      "Iteration 4500, loss: 0.08858858793973923, remaining: 2s\n",
      "Accuracy on the test set: 97.77%\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "model.train()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "for i in range(5000):\n",
    "\tix = torch.randint(0, x.shape[0], (32,))\n",
    "\txb, yb = x[ix], y[ix]\n",
    "\tlogits = model(xb)\n",
    "\tloss = torch.nn.functional.cross_entropy(logits, yb.argmax(dim=1))\n",
    "\toptimizer.zero_grad()\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\tif i % 500 == 0:\n",
    "\t\tremaining = (time() - start) * (5000 - i) / (i + 1)\t\t\n",
    "\t\tprint(f'Iteration {i}, loss: {loss.item()}, remaining: {remaining:.0f}s')\n",
    "\n",
    "model.eval()\n",
    "accuracy = (model(xt).argmax(dim=1) == yt.argmax(dim=1)).float().mean().item()\n",
    "print(f'Accuracy on the test set: {100 * accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much better! We have achieved ~98% accuracy with the same number of parameters, just be taking advantage of using a better architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
