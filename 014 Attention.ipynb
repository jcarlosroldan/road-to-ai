{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `014` Attention mechanisms\n",
    "\n",
    "Requirements: 010 Embeddings, 013 LSTM\n",
    "\n",
    "⚠️ WIP\n",
    "\n",
    "There is a fundamental problem with LSTM, GRU and other RNNs, which is that they are not feedforward, so that the individual timesteps of processing a sequence cannot be parallelized. This time dependency makes training slower. RNNs typically capture dependencies 100\n",
    "\n",
    "There is a different kind of layer called `Attention` that can be used to solve this problem. The idea is to have a layer that can look at the entire sequence at once and decide which parts of the sequence are important for the current timestep. This way, the layer can be parallelized and can capture long-term dependencies. The underlying idea is converting every element in the sequence into a linear combination of all the elements in the sequence, with the weights of the linear combination being learned. Let's see an implementation in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads\n",
    "from matplotlib import pyplot as plt\n",
    "from string import ascii_letters, digits\n",
    "from time import time\n",
    "from unicodedata import category, normalize\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "vocabulary = ascii_letters + digits + ' .,;\\'!'\n",
    "c2i = {c: i for i, c in enumerate(vocabulary)}\n",
    "i2c = {i: c for i, c in enumerate(vocabulary)}\n",
    "\n",
    "def vectorize_sentence(s):\n",
    "\treturn [c2i[c] for c in normalize('NFD', s) if category(c) != 'Mn' and c in vocabulary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor([vectorize_sentence('Hello, World!')])  # (batch_size=1, context_length=13)\n",
    "\n",
    "embedding_channels = 8\n",
    "embeddings = torch.randn(len(vocabulary), embedding_channels)  # (vocabulary_size, embedding_channels=8)\n",
    "input = embeddings[input]  # (batch_size=1, context_length=13, embedding_channels=8)\n",
    "\n",
    "head_size = 16\n",
    "# key space mapping: what each has to offer\n",
    "W_k = torch.randn(embedding_channels, head_size)  # (embedding_channels=8, head_size=16)\n",
    "# query space mapping: what each token is looking for\n",
    "W_q = torch.randn(embedding_channels, head_size)  # (embedding_channels=8, head_size=16)\n",
    "\n",
    "k = input @ W_k  # (batch_size=1, context_length=13, head_size=16)\n",
    "q = input @ W_q  # (batch_size=1, context_length=13, head_size=16)\n",
    "\n",
    "# dot product of what each token is looking for and what each has to offer -> attention weights: how much each token should pay attention to each other token\n",
    "# note that you swap the last two dimensions of k to make a square matrix of dot products\n",
    "attention_weights = q @ k.transpose(-2, -1)  # (batch_size=1, context_length=13, context_length=13)\n",
    "# now, divide it by the square root of the head size to make the dot products more stable\n",
    "attention_weights /= head_size ** 0.5\n",
    "# apply a softmax to make the attention weights sum to 1\n",
    "attention_weights = attention_weights.softmax(dim=-1)  # (batch_size=1, context_length=13, context_length=13)\n",
    "\n",
    "# value space: what each token is contributing\n",
    "W_v = torch.randn(embedding_channels, head_size)  # (embedding_channels=8, head_size=16)\n",
    "\n",
    "v = input @ W_v  # (batch_size=1, context_length=13, head_size=16)\n",
    "\n",
    "# weighted sum of what each token is contributing, based on how much each token should pay attention to each other token\n",
    "output = attention_weights @ v  # (batch_size=1, context_length=13, head_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6752415 sentences from 8 languages using 68 different characters\n",
      "tensor([11,  8, 13,  6, 64, 62,  0, 18, 62, 11, 14, 22, 62,  0, 18, 62],\n",
      "       device='cuda:0') tensor(4, device='cuda:0') -> ling, as low as  en\n"
     ]
    }
   ],
   "source": [
    "with open('custom-data/sentences.json', encoding='utf-8') as f:\n",
    "\tdata = loads(f.read())\n",
    "\n",
    "languages = list(data.keys())\n",
    "\n",
    "X, Y = [], []\n",
    "block_size = 16\n",
    "for language, sentences in data.items():\n",
    "\tfor sentence in sentences:\n",
    "\t\tsentence = vectorize_sentence(sentence)\n",
    "\t\tfor i in range(len(sentence) - block_size + 1):\n",
    "\t\t\tX.append(sentence[i:i+block_size])\n",
    "\t\t\tY.append(languages.index(language))\n",
    "ix = torch.randperm(len(X))\n",
    "X = torch.tensor([X[i] for i in ix], device=device)\n",
    "Y = torch.tensor([Y[i] for i in ix], device=device)\n",
    "\n",
    "print(f'Loaded {len(X)} sentences from {len(languages)} languages using {len(vocabulary)} different characters')\n",
    "print(X[0], Y[0], '->', ''.join(i2c[i.item()] for i in X[0]), languages[Y[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 16, 20])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mh = torch.nn.MultiheadAttention(embed_dim=20, num_heads=5)\n",
    "x = torch.randn(32, 16, 20)\n",
    "mh(x, x, x, need_weights=False)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model with 285,960 parameters\n"
     ]
    }
   ],
   "source": [
    "class SimpleAttention(torch.nn.Module):\n",
    "\tdef __init__(self, embedding_channels, head_size):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.W_k = torch.nn.Linear(embedding_channels, head_size, bias=False)\n",
    "\t\tself.W_q = torch.nn.Linear(embedding_channels, head_size, bias=False)\n",
    "\t\tself.W_v = torch.nn.Linear(embedding_channels, head_size, bias=False)\n",
    "\tdef forward(self, x):\n",
    "\t\tk = self.W_k(x)\n",
    "\t\tq = self.W_q(x)\n",
    "\t\tattention = q @ k.transpose(-2, -1) / k.size(-1) ** 0.5\n",
    "\t\tattention = attention.softmax(dim=-1)\n",
    "\t\tv = self.W_v(x)\n",
    "\t\treturn attention @ v\n",
    "\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "\tdef __init__(self, embedding_channels, num_heads, head_size):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.heads = torch.nn.ModuleList([\n",
    "\t\t\tSimpleAttention(embedding_channels, head_size)\n",
    "\t\t\tfor _ in range(num_heads)\n",
    "\t\t])\n",
    "\t\tself.fc = torch.nn.Linear(embedding_channels, num_heads * head_size)\n",
    "\tdef forward(self, x):\n",
    "\t\tx = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\t\treturn self.fc(x)\n",
    "\t\n",
    "class LanguageClassifier(torch.nn.Module):\n",
    "\tdef __init__(self, vocabulary_size, output_classes, block_size=16, embedding_channels=256, num_heads=4, head_size=64):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.character_embedding = torch.nn.Embedding(vocabulary_size, embedding_channels)\n",
    "\t\tself.positional_embedding = torch.nn.Embedding(block_size, embedding_channels)\n",
    "\t\tself.attention = MultiHeadAttention(embedding_channels, num_heads, head_size)\n",
    "\t\tself.fc = torch.nn.Linear(embedding_channels, output_classes)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tch_emb = self.character_embedding(x)\n",
    "\t\tpos_emb = self.positional_embedding(torch.arange(x.size(-1), device=x.device))\n",
    "\t\tx = ch_emb + pos_emb\n",
    "\t\tx = self.attention(x)\n",
    "\t\tx = x.mean(dim=-2)\n",
    "\t\treturn self.fc(x)\n",
    "\n",
    "model = LanguageClassifier(len(vocabulary), len(languages)).to(device)\n",
    "print(f'Created model with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with default learning rate\n",
      "Epoch   50 loss: 37.4759 remaining: 1:37\n",
      "Epoch 1000 loss: 2.5299 remaining: 1:07\n",
      "Epoch 2000 loss: 2.2777 remaining: 0:59\n",
      "Epoch 3000 loss: 2.1330 remaining: 0:51\n",
      "Epoch 4000 loss: 29.3490 remaining: 0:42\n",
      "Epoch 5000 loss: 6.6378 remaining: 0:35\n",
      "Epoch 6000 loss: 8.1393 remaining: 0:28\n",
      "Epoch 7000 loss: 4.5081 remaining: 0:21\n",
      "Epoch 8000 loss: 4.1079 remaining: 0:14\n",
      "Epoch 9000 loss: 23937.5566 remaining: 0:07\n",
      "Training with lower learning rate\n",
      "Epoch   50 loss: 13.9941 remaining: 1:13\n",
      "Epoch 1000 loss: 5.7585 remaining: 1:01\n",
      "Epoch 2000 loss: 4.1984 remaining: 0:55\n",
      "Epoch 3000 loss: 3.6367 remaining: 0:49\n",
      "Epoch 4000 loss: 2.5037 remaining: 0:42\n",
      "Epoch 5000 loss: 2.9127 remaining: 0:36\n",
      "Epoch 6000 loss: 2.3875 remaining: 0:29\n",
      "Epoch 7000 loss: 2.7092 remaining: 0:21\n",
      "Epoch 8000 loss: 2.0104 remaining: 0:14\n",
      "Epoch 9000 loss: 2.7479 remaining: 0:07\n"
     ]
    }
   ],
   "source": [
    "def train(model, iterations=10000, batch_size=32, lr=0.05):\n",
    "\tstart = time()\n",
    "\tcriterion = torch.nn.CrossEntropyLoss()\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\tlosses = []\n",
    "\tfor i in range(iterations):\n",
    "\t\tix = torch.randint(len(X), (batch_size,))\n",
    "\t\txb, yb = X[ix], Y[ix]\n",
    "\t\tloss = criterion(model(xb), yb)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\tlosses.append(loss.item())\n",
    "\t\tif i % 1000 == 0 and i > 0 or i == 50:\n",
    "\t\t\tremaining = (time() - start) * (iterations - i) / (i + 1)\n",
    "\t\t\tprint(f'Epoch {i:4} loss: {loss.item():.4f} remaining: {remaining//60:.0f}:{remaining%60:02.0f}')\n",
    "\treturn losses\n",
    "\n",
    "print('Training with default learning rate')\n",
    "losses = train(model)\n",
    "print('Training with lower learning rate')\n",
    "losses.extend(train(model, lr=0.005))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
