{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `018` Reinforcement Learning\n",
    "\n",
    "Requirements: 016 Transformers\n",
    "\n",
    "⚡⚡⚡⚡WIP⚡⚡⚡⚡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from random import choice\n",
    "\n",
    "device = torch.device('cuda' if torch.backends.cudnn.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an environment for the game. We can do this in many different ways, but we'll use this specific class-based format to make it more similar to the widespread gymnasium format. Although we'll implement our own RL setup, environments with this format can be used interchangeably with many RL libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeEnv:\n",
    "\tWIN_MASKS = (0, 1, 2), (3, 4, 5), (6, 7, 8), (0, 3, 6), (1, 4, 7), (2, 5, 8), (0, 4, 8), (2, 4, 6)\n",
    "\tdef reset(self):\n",
    "\t\tself.state = torch.zeros(9, device=device, dtype=int)\n",
    "\t\tself.mask = torch.ones(9, device=device, dtype=torch.uint8)\n",
    "\t\tself.player = 0\n",
    "\n",
    "\tdef step(self, action):\n",
    "\t\tassert self.state[action] == 0, f'Invalid action: {action} in state: {self.state}'\n",
    "\t\tself.state[action] = self.player + 1\n",
    "\t\tfor a, b, c in self.WIN_MASKS:\n",
    "\t\t\tif self.state[a] == self.state[b] == self.state[c] == self.player + 1:\n",
    "\t\t\t\tself.reward = 1\n",
    "\t\t\t\tself.done = 1\n",
    "\t\t\t\tbreak\n",
    "\t\telse:\n",
    "\t\t\tself.reward = 0\n",
    "\t\t\tself.mask = (self.state == 0).to(torch.uint8)\n",
    "\t\t\tself.done = not self.mask.any()\n",
    "\t\tself.player = 1 - self.player\n",
    "\n",
    "\tdef render(self):\n",
    "\t\tfor row in range(3):\n",
    "\t\t\tprint(''.join('·XO'[self.state[row * 3 + col]] for col in range(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test the environment with two random agents. We will define a random agent that will return a random probability distribution over the actions. Then, we will define a method to run a full episode while keeping track of the state (observations), actions and rewards. This method will have a parameter epsilon that will smooth the action probabilities, allowing the agent to explore the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "···\n",
      "···\n",
      "···\n",
      "\n",
      "Turn 0, player 0:\n",
      "···\n",
      "···\n",
      "··X\n",
      "Reward: 0, Done: False\n",
      "\n",
      "Turn 1, player 1:\n",
      "···\n",
      "O··\n",
      "··X\n",
      "Reward: 0, Done: False\n",
      "\n",
      "Turn 2, player 0:\n",
      "···\n",
      "O·X\n",
      "··X\n",
      "Reward: 0, Done: False\n",
      "\n",
      "Turn 3, player 1:\n",
      "···\n",
      "O·X\n",
      "O·X\n",
      "Reward: 0, Done: False\n",
      "\n",
      "Turn 4, player 0:\n",
      "·X·\n",
      "O·X\n",
      "O·X\n",
      "Reward: 0, Done: False\n",
      "\n",
      "Turn 5, player 1:\n",
      "·X·\n",
      "O·X\n",
      "OOX\n",
      "Reward: 0, Done: False\n",
      "\n",
      "Turn 6, player 0:\n",
      "·X·\n",
      "OXX\n",
      "OOX\n",
      "Reward: 0, Done: False\n",
      "\n",
      "Turn 7, player 1:\n",
      "·XO\n",
      "OXX\n",
      "OOX\n",
      "Reward: 0, Done: False\n",
      "\n",
      "Turn 8, player 0:\n",
      "XXO\n",
      "OXX\n",
      "OOX\n",
      "Reward: 1, Done: 1\n"
     ]
    }
   ],
   "source": [
    "class RandomAgent:\n",
    "\tdef __call__(self, _):\n",
    "\t\treturn torch.randn(9, device=device)\n",
    "\n",
    "def play_episode(env, players, epsilon=0, render=False):\n",
    "\tobservations, rewards, actions = [], [], []\n",
    "\tenv.reset()\n",
    "\tif render: env.render()\n",
    "\tturn = 0\n",
    "\twhile True:\n",
    "\t\tplayer = turn % len(players)\n",
    "\t\tif render: print(f'\\nTurn {turn}, player {player}:')\n",
    "\t\tobservations.append(env.state.clone())\n",
    "\t\tlogits = players[player](env.state)\n",
    "\t\tprobs = (logits.softmax(-1) + epsilon) * env.mask\n",
    "\t\taction = torch.multinomial(probs, 1).item()\n",
    "\t\tactions.append(action)\n",
    "\t\tenv.step(action)\n",
    "\t\trewards.append(env.reward)\n",
    "\t\tif render:\n",
    "\t\t\tenv.render()\n",
    "\t\t\tprint(f'Reward: {env.reward}, Done: {env.done}')\n",
    "\t\tif env.done: break\n",
    "\t\tturn += 1\n",
    "\treturn observations, rewards, actions\n",
    "\n",
    "play_episode(TicTacToeEnv(), (RandomAgent(), RandomAgent()), render=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player(torch.nn.Module):\n",
    "\tdef __init__(self, emb_dim=128, hidden_dim=128, num_blocks=4, num_heads=4):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.embedding = BoardEmbeding(emb_dim)\n",
    "\t\tself.hidden = torch.nn.Sequential(*[TransformerBlock(hidden_dim, num_heads) for _ in range(num_blocks)]).to(device)\n",
    "\t\tself.out = torch.nn.Linear(hidden_dim, 1, device=device)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.embedding(x)\n",
    "\t\tx = self.hidden(x)\n",
    "\t\tx = self.out(x)\n",
    "\t\treturn x.squeeze(-1)\n",
    "\n",
    "class BoardEmbeding(torch.nn.Module):\n",
    "\tdef __init__(self, emb_dim):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.piece_embedding = torch.nn.Parameter(torch.randn(3, emb_dim, device=device))\n",
    "\t\tself.pos_embedding = torch.nn.Parameter(torch.randn(9, emb_dim, device=device))\n",
    "\t\tself.register_buffer('pos', torch.arange(9, device=device))\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx= self.piece_embedding[x] + self.pos_embedding[self.pos]\n",
    "\t\treturn x\n",
    "\n",
    "class TransformerBlock(torch.nn.Module):\n",
    "\tdef __init__(self, channels, num_heads, dropout=.1):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.norm1 = torch.nn.LayerNorm(channels)\n",
    "\t\tself.attn = torch.nn.MultiheadAttention(channels, num_heads, dropout)\n",
    "\t\tself.ff = torch.nn.Sequential(\n",
    "\t\t\ttorch.nn.Linear(channels, 4 * channels),\n",
    "\t\t\ttorch.nn.GELU(),\n",
    "\t\t\ttorch.nn.Linear(4 * channels, channels)\n",
    "\t\t)\n",
    "\t\tself.norm2 = torch.nn.LayerNorm(channels)\n",
    "\t\tself.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.norm1(x)\n",
    "\t\tx = x + self.attn(x, x, x)[0]\n",
    "\t\tx = self.norm2(x)\n",
    "\t\tx = x + self.ff(x)\n",
    "\t\tx = self.dropout(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0], device='mps:0'),\n",
       " tensor([0, 0, 0, 0, 0, 1, 0, 0, 0], device='mps:0'),\n",
       " tensor([0, 0, 2, 0, 0, 1, 0, 0, 0], device='mps:0'),\n",
       " tensor([1, 0, 2, 0, 0, 1, 0, 0, 0], device='mps:0'),\n",
       " tensor([1, 0, 2, 2, 0, 1, 0, 0, 0], device='mps:0'),\n",
       " tensor([1, 1, 2, 2, 0, 1, 0, 0, 0], device='mps:0'),\n",
       " tensor([1, 1, 2, 2, 2, 1, 0, 0, 0], device='mps:0'),\n",
       " tensor([1, 1, 2, 2, 2, 1, 1, 0, 0], device='mps:0'),\n",
       " tensor([1, 1, 2, 2, 2, 1, 1, 2, 0], device='mps:0')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = Player()\n",
    "p2 = Player()\n",
    "observations, rewards, actions = play_episode(TicTacToeEnv(), (p1, p2))\n",
    "observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 2, 0, 3, 1, 4, 6, 7, 8]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTINUE FROM HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = BoardTransformer(4).to(device)\n",
    "print(f'Number of parameters in the policy net: {sum(p.numel() for p in policy.parameters()):,}')\n",
    "print(policy(new_board()))\n",
    "\n",
    "value = BoardTransformer(1).to(device)\n",
    "print(f'Number of parameters in the value net: {sum(p.numel() for p in value.parameters()):,}')\n",
    "print(value(new_board()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1      2      4      2\n",
      "    8     32     64      4\n",
      "    4     16     32      2\n",
      "    1      2      2      2\n"
     ]
    }
   ],
   "source": [
    "history = play_episode(policy)\n",
    "display_board(history['states'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\tdef __init__(self, maxlen):\n",
    "\t\tself.states = torch.empty((maxlen, 16, 4, 4), device=device)\n",
    "\t\tself.actions = torch.empty((maxlen,), dtype=torch.long, device=device)\n",
    "\t\tself.rewards = torch.empty((maxlen,), device=device)\n",
    "\t\tself.logits = torch.empty((maxlen, 4), device=device)\n",
    "\t\tself.append_idx = 0\n",
    "\t\tself.cycled = False\n",
    "\t\n",
    "\tdef append(self, state, action, reward, logits):\n",
    "\t\tself.states[self.append_idx] = state\n",
    "\t\tself.actions[self.append_idx] = action\n",
    "\t\tself.rewards[self.append_idx] = reward\n",
    "\t\tself.logits[self.append_idx] = logits\n",
    "\t\tself.append_idx += 1\n",
    "\t\tif self.append_idx == len(self.states):\n",
    "\t\t\tself.append_idx = 0\n",
    "\t\t\tself.cycled = True\n",
    "\t\n",
    "\tdef sample(self, batch_size):\n",
    "\t\tix = torch.randint(0, len(self.states) if self.cycled else self.append_idx, (batch_size,))\n",
    "\t\treturn self.states[ix], self.actions[ix], self.rewards[ix], self.logits[ix]\n",
    "\n",
    "buffer = ReplayBuffer(100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m \t\t\tbuffer\u001b[38;5;241m.\u001b[39mappend(state, action, reward, logits)\n\u001b[0;32m     32\u001b[0m \t\tupdate_policy(policy_net, value_net, buffer, optimizer_policy, optimizer_value)\n\u001b[1;32m---> 34\u001b[0m \u001b[43mtrain_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 32\u001b[0m, in \u001b[0;36mtrain_policy\u001b[1;34m(policy_net, value_net, buffer, num_episodes, lr)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state, action, reward, logits \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(episode_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m'\u001b[39m], episode_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m'\u001b[39m], episode_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrewards\u001b[39m\u001b[38;5;124m'\u001b[39m], episode_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m     31\u001b[0m \tbuffer\u001b[38;5;241m.\u001b[39mappend(state, action, reward, logits)\n\u001b[1;32m---> 32\u001b[0m \u001b[43mupdate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m, in \u001b[0;36mupdate_policy\u001b[1;34m(policy_net, value_net, buffer, optimizer_policy, optimizer_value, batch_size, epochs, gamma, clip)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      7\u001b[0m \tstates, actions, rewards, logits \u001b[38;5;241m=\u001b[39m buffer\u001b[38;5;241m.\u001b[39msample(batch_size)\n\u001b[1;32m----> 8\u001b[0m \tadvantages \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_advantages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \tlog_probs_old \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlog()\n\u001b[0;32m     10\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m state, action, reward, log_prob_old \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(states, actions, rewards, log_probs_old):\n",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m, in \u001b[0;36mcompute_advantages\u001b[1;34m(states, rewards, value_net, gamma)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_advantages\u001b[39m(states, rewards, value_net, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m \tvalues \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m rewards \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m values[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m-\u001b[39m values[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\juanc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\juanc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m, in \u001b[0;36mBoardTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 9\u001b[0m \tx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \tx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden(x)\n\u001b[0;32m     11\u001b[0m \tx \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\juanc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\juanc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m, in \u001b[0;36mBoardEmbeding.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m \texp_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \tx_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_x_embedding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_x)\n\u001b[0;32m     13\u001b[0m \ty_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_y_embedding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_y)\n",
      "File \u001b[1;32mc:\\Users\\juanc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\juanc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\juanc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\juanc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "def compute_advantages(states, rewards, value_net, gamma=0.99):\n",
    "\tvalues = value_net(states)\n",
    "\treturn rewards + gamma * values[1:] - values[:-1]\n",
    "\n",
    "def update_policy(policy_net, value_net, buffer, optimizer_policy, optimizer_value, batch_size=64, epochs=4, gamma=0.99, clip=0.2):\n",
    "\tfor _ in range(epochs):\n",
    "\t\tstates, actions, rewards, logits = buffer.sample(batch_size)\n",
    "\t\tadvantages = compute_advantages(states, rewards, value_net, gamma)\n",
    "\t\tlog_probs_old = logits.gather(1, actions.unsqueeze(-1)).squeeze(-1).log()\n",
    "\t\tfor state, action, reward, log_prob_old in zip(states, actions, rewards, log_probs_old):\n",
    "\t\t\toptimizer_policy.zero_grad()\n",
    "\t\t\toptimizer_value.zero_grad()\n",
    "\t\t\tlogits_new = policy_net(state.unsqueeze(0)).squeeze(0)\n",
    "\t\t\tlog_probs_new = logits_new.gather(0, action).log()\n",
    "\t\t\tratio = (log_probs_new - log_prob_old).exp()\n",
    "\t\t\tsurr1 = ratio * reward\n",
    "\t\t\tsurr2 = torch.clamp(ratio, 1.0 - clip, 1.0 + clip) * reward\n",
    "\t\t\tpolicy_loss = -torch.min(surr1, surr2).mean()\n",
    "\t\t\tpolicy_loss.backward()\n",
    "\t\t\toptimizer_policy.step()\n",
    "\t\t\tvalue_loss = ((value_net(state.unsqueeze(0)) - reward) ** 2).mean()\n",
    "\t\t\tvalue_loss.backward()\n",
    "\t\t\toptimizer_value.step()\n",
    "\n",
    "def train_policy(policy_net, value_net, buffer, num_episodes, lr=1e-4):\n",
    "\toptimizer_policy = torch.optim.Adam(policy.parameters(), lr=lr)\n",
    "\toptimizer_value = torch.optim.Adam(value.parameters(), lr=lr)\n",
    "\tfor _ in range(num_episodes):\n",
    "\t\tepisode_data = play_episode(policy_net)\n",
    "\t\tfor state, action, reward, logits in zip(episode_data['states'], episode_data['actions'], episode_data['rewards'], episode_data['logits']):\n",
    "\t\t\tbuffer.append(state, action, reward, logits)\n",
    "\t\tupdate_policy(policy_net, value_net, buffer, optimizer_policy, optimizer_value)\n",
    "\n",
    "train_policy(policy, value, buffer, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
