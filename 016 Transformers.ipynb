{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `016` Transformers\n",
    "\n",
    "Requirements: 014 Attention and dropout, 015 Residual connections\n",
    "\n",
    "☢️☢️ WIP ☢️☢️\n",
    "\n",
    "Attention mechanisms were proposed as a way to make the RNN inputs contain information about other terms in the sequence. This way, every sequence element can be contextualized properly with the information from other tokens. However, an architecture called transformer proposed by [Vaswani et al., 2017](ttps://arxiv.org/pdf/2002.04745v1.pdf) took the world by surprise.\n",
    "\n",
    "Basically, he removed the RNN layers and used just a bunch of linear layers, attention mechanisms, residual connections and normalization. Applied in the context of German to English translation, the architecture achieved better quality (BLEU score) than any previous model. Furthermore, applying it over general text corpuses created a level of generalization pretty impressive and scalable with model size.\n",
    "\n",
    "In this notebook I will define a transformer block, build a model with many of them, and train it over a corpus of educational content called FineWeb-EDU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(torch.nn.Module):\n",
    "\tdef __init__(self, embed_dim, num_heads, dropout):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.attention_heads = torch.nn.MultiheadAttention(embed_dim, num_heads, dropout)\n",
    "\t\tself.norm1 = torch.nn.RMSNorm(embed_dim)\n",
    "\t\tself.ff = torch.nn.Linear(embed_dim, embed_dim)\n",
    "\t\tself.norm2 = torch.nn.RMSNorm(embed_dim)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = x + self.attention_heads(x)\n",
    "\t\tx = self.norm1(x)\n",
    "\t\tx = x + self.ff(x)\n",
    "\t\tx = torch.relu(x)\n",
    "\t\tx = self.norm2(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "\tdef __init__(self, num_embeddings, output_dim, embed_dim=32, num_blocks=4, num_heads=4, dropout=.2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.tok_embed = torch.nn.Embedding(num_embeddings, embed_dim)\n",
    "\t\tself.pos_embed = torch.nn.Embedding(num_embeddings, embed_dim)\n",
    "\t\tself.blocks = torch.nn.Sequential([\n",
    "\t\t\tTransformerBlock(embed_dim, num_heads, dropout)\n",
    "\t\t\tfor _ in range(num_blocks)\n",
    "\t\t])\n",
    "\t\tself.output = torch.nn.Linear(embed_dim, output_dim)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.tok_embed(x) + self.pos_embed(torch.arange(x.size(-1)))\n",
    "\t\tx = self.blocks(x)\n",
    "\t\tx = self.output(x)\n",
    "\t\treturn x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
